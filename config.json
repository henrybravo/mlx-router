{
    "defaults": {
      "max_tokens": 16384,
      "timeout": 120,
      "cache_size": 2,
      "memory_threshold_gb": 5.0,
      "safety_margin": 0.9,
      "swap_critical_percent": 99.0,
      "swap_high_percent": 90.0,
      "stream": true,
      "stream_chunk_size": 4,
      "streaming_format": "sse",
      "warmup_tokens": 5,
      "enable_function_calling": true,
      "model": "mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit",       
      "model_directory": "/Users/username/models"
    },
    "server": {
        "ip": "0.0.0.0",
        "port": 8800,
        "debug": false
    },
    "models": {
        "mlx-community/chandra-8bit": {
            "max_tokens": 8192,
            "temp": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "min_p": 0.05,
            "chat_template": "generic",
            "required_memory_gb": 4,
            "supports_tools": false,
            "supports_vision": true
        },
        "mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit": {
            "max_tokens": 16384,
            "temp": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "min_p": 0.05,
            "chat_template": "generic",
            "reasoning_response": "enable",
            "required_memory_gb": 40,
            "memory_pressure_max_tokens": {
                    "normal": 16384,
                    "moderate": 16384,
                    "high": 16384,
                    "critical": 8192
            }
        },
        "mlx-community/gpt-oss-120b-MXFP4-Q8": {
            "max_tokens": 16384,
            "temp": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "min_p": 0.05,
            "chat_template": "gpt-oss",
            "reasoning_response": "disable",
            "required_memory_gb": 8,
            "memory_pressure_max_tokens": {
                    "normal": 16384,
                    "moderate": 16384,
                    "high": 16384,
                    "critical": 8192
            }
        },
        "mlx-community/Llama-3.3-70B-Instruct-4bit": {
            "max_tokens": 16384,
            "temp": 0.7,
            "top_p": 0.95,
            "top_k": 50,
            "min_p": 0.05,
            "chat_template": "llama3",
            "required_memory_gb": 40,
            "supports_tools": true,
            "memory_pressure_max_tokens": {
                "normal": 12288,
                "moderate": 8192,
                "high": 8192,
                "critical": 4096
            }
        },
        "deepseek-ai/deepseek-coder-6.7b-instruct": {
            "max_tokens": 8192,
            "temp": 0.1,
            "top_p": 0.95,
            "top_k": 20,
            "min_p": 0.1,
            "chat_template": "deepseek",
            "required_memory_gb": 8,
            "supports_tools": true,
            "memory_pressure_max_tokens": {
                "normal": 12288,
                "moderate": 8192,
                "high": 8192,
                "critical": 4096
            }
        },
        "mlx-community/Phi-4-reasoning-plus-6bit": {
            "max_tokens": 8192,
            "temp": 0.3,
            "top_p": 0.9,
            "top_k": 25,
            "min_p": 0.08,
            "chat_template": "phi4",
            "required_memory_gb": 12,
            "supports_tools": true,
            "memory_pressure_max_tokens": {
                "normal": 12288,
                "moderate": 8192,
                "high": 8192,
                "critical": 4096
            }
        },
        "mlx-community/Qwen3-30B-A3B-8bit": {
            "max_tokens": 16384,
            "temp": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "min_p": 0.05,
            "chat_template": "qwen",
            "required_memory_gb": 18,
            "supports_tools": true,
            "memory_pressure_max_tokens": {
                "normal": 12288,
                "moderate": 8192,
                "high": 8192,
                "critical": 4096
            }
        }
    }
}